# Azure-Data-Integration

This project demonstrates the development of an end-to-end data integration and processing pipeline using Azure services. It includes the ingestion of over 10,000 records from local files to Azure Blob Storage and Azure SQL Database via Azure Data Factory, ensuring efficient and seamless data flow between services. 

A schedule trigger was configured to automate pipeline runs every 3 minutes, achieving consistent data processing with zero data loss. Additionally, the project implements bidirectional data replication between ADLS Gen2 instances in Canada Central and West Europe, synchronizing over 500 files per hour using event-based triggers. Advanced Azure components, including Azure Data Lake, Databricks, and Synapse Analytics, were utilized to process and transform datasets exceeding 50GB, optimizing workflows for large-scale analytics tasks. 

This project showcases expertise in cloud-based data integration, automation, and advanced analytics for high-performance and scalable solutions.

